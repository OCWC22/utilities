{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OCWC22/utilities/blob/main/deepseek_ppt_to_markdown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c044bd1",
      "metadata": {
        "id": "4c044bd1"
      },
      "source": [
        "\n",
        "# DeepSeek-OCR: PPT/PDF → Markdown with ASCII diagrams\n",
        "\n",
        "End-to-end: **PPT/PPTX/PDF → images → DeepSeek-OCR → `slides.md`**  \n",
        "Plus **ASCII fallbacks** so figures are always visible inside Markdown (no Mermaid).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02c814cb",
      "metadata": {
        "id": "02c814cb"
      },
      "source": [
        "\n",
        "## Quick start\n",
        "1) Set `INPUT_FILE` to your deck (`.ppt`, `.pptx`, or `.pdf`).  \n",
        "2) Run **Setup** → **Load model** → **Convert & OCR**.  \n",
        "3) Grab `output/slides.md` and any cropped images in `output/figures/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "180e8972",
      "metadata": {
        "id": "180e8972"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "    display(HTML('''\n",
        "\n",
        "    '''))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c7b6f6",
      "metadata": {
        "id": "f7c7b6f6"
      },
      "outputs": [],
      "source": [
        "# === Config ===\n",
        "INPUT_FILE = \"Chapter 11.pdf\"   # .ppt/.pptx/.pdf\n",
        "OUTPUT_DIR = \"output\"\n",
        "\n",
        "# Use the model only for OCR/text; we still ask it to try ASCII diagrams, but\n",
        "# literal pixel→ASCII fallbacks below guarantee diagrams show up.\n",
        "RECONSTRUCT_ASCII = True             # Let the model attempt clean ASCII diagrams\n",
        "MODEL_NAME = \"deepseek-ai/DeepSeek-OCR\"\n",
        "\n",
        "# Precision / memory\n",
        "USE_BF16_FLASH_ATTN = True           # Good perf on recent NVIDIA GPUs\n",
        "\n",
        "# ASCII fallbacks (literal pixel→ASCII) — accurate, always visible in MD\n",
        "ASCII_FOR_FIGURES = True             # ASCII for each model-cropped figure\n",
        "ASCII_FOR_FULL_SLIDE = False         # Also ASCII for whole slide image\n",
        "ASCII_FIGURE_WIDTH = 100             # 100–160 = more detail\n",
        "ASCII_SLIDE_WIDTH  = 140\n",
        "ASCII_CHARSET = \" .:-=+*#%@\"         # dark→light ramp (pure ASCII)\n",
        "ASCII_Y_ASPECT = 0.5                 # characters are taller than wide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b2d95c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "3b2d95c4",
        "outputId": "1d163aee-76eb-462c-e5ef-85e4a262e113"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> apt-get update -y\n",
            "> apt-get install -y poppler-utils libreoffice\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# === Setup: system & Python deps ===\n",
        "# Notes:\n",
        "# - DeepSeek-OCR expects an NVIDIA GPU with CUDA + Flash-Attention 2.\n",
        "# - We install pdf2image (needs Poppler). On Linux (Colab), we apt-get poppler.\n",
        "# - For PPT/PPTX, we try headless LibreOffice to convert to PDF.\n",
        "\n",
        "import shutil, subprocess, sys\n",
        "\n",
        "def _run(cmd):\n",
        "    print(\">\", \" \".join(cmd))\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True)\n",
        "    except Exception as e:\n",
        "        print(\"[warn]\", e)\n",
        "\n",
        "# Linux (Colab-like) helpers\n",
        "if shutil.which(\"apt-get\"):\n",
        "    _run([\"apt-get\", \"update\", \"-y\"])   # no sudo inside many notebooks\n",
        "    _run([\"apt-get\", \"install\", \"-y\", \"poppler-utils\", \"libreoffice\"])  # for pdf2image + PPT→PDF\n",
        "\n",
        "# Python deps\n",
        "!pip uninstall -y bitsandbytes || true\n",
        "!pip uninstall -y triton || true\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install torch torchvision --index-url https://download.pytorch.org/whl/cu121 || true\n",
        "!pip -q install transformers==4.46.3 tokenizers==0.20.3 einops addict easydict pillow tqdm pdf2image python-pptx\n",
        "!pip -q install flash-attn==2.7.3 --no-build-isolation || true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34012f2",
      "metadata": {
        "id": "c34012f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Utilities: PPT/PPTX → PDF, PDF → images, image → ASCII ===\n",
        "from pathlib import Path\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import shutil, subprocess\n",
        "\n",
        "def ensure_dir(p: Path):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def ppt_to_pdf(ppt_path: Path, out_dir: Path) -> Path:\n",
        "    ensure_dir(out_dir)\n",
        "    soffice = shutil.which(\"soffice\") or shutil.which(\"libreoffice\")\n",
        "    if not soffice:\n",
        "        raise RuntimeError(\"LibreOffice not found; supply a PDF directly or install LibreOffice.\")\n",
        "    cmd = [soffice, \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", str(out_dir), str(ppt_path)]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "    pdf_path = out_dir / (ppt_path.stem + \".pdf\")\n",
        "    if not pdf_path.exists():\n",
        "        alt = out_dir / (ppt_path.stem + \".PDF\")\n",
        "        if alt.exists():\n",
        "            pdf_path = alt\n",
        "    if not pdf_path.exists():\n",
        "        raise FileNotFoundError(\"PPT/PPTX → PDF failed.\")\n",
        "    return pdf_path\n",
        "\n",
        "def pdf_to_images(pdf_path: Path, dpi: int = 300, img_dir: Path = Path(\"slides\")) -> list[Path]:\n",
        "    ensure_dir(img_dir)\n",
        "    pages = convert_from_path(str(pdf_path), dpi=dpi)\n",
        "    outs = []\n",
        "    for i, page in enumerate(pages, 1):\n",
        "        out = img_dir / f\"slide_{i:03d}.png\"\n",
        "        page.save(out, \"PNG\")\n",
        "        outs.append(out)\n",
        "    return outs\n",
        "\n",
        "def image_to_ascii(img_path, width=120, charset=\" .:-=+*#%@\", y_aspect=0.5):\n",
        "    im = Image.open(img_path).convert(\"L\")\n",
        "    w = int(width)\n",
        "    h = max(1, int(im.height * (w / im.width) * y_aspect))\n",
        "    im = im.resize((w, h))\n",
        "    arr = np.array(im)\n",
        "    scale = (len(charset) - 1) / 255.0\n",
        "    lines = []\n",
        "    for row in arr:\n",
        "        lines.append(\"\".join(charset[int(px * scale)] for px in row))\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6f011c",
      "metadata": {
        "id": "9a6f011c"
      },
      "outputs": [],
      "source": [
        "# === Load DeepSeek-OCR ===\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "_tokenizer_kwargs = {\"trust_remote_code\": True}\n",
        "_model_kwargs = {\n",
        "    \"trust_remote_code\": True,\n",
        "    \"use_safetensors\": True,\n",
        "}\n",
        "\n",
        "has_cuda = torch.cuda.is_available()\n",
        "use_flash_attn = USE_BF16_FLASH_ATTN and has_cuda\n",
        "if USE_BF16_FLASH_ATTN and not has_cuda:\n",
        "    print(\"[warn] CUDA not detected; disabling Flash Attention 2 for CPU fallback.\")\n",
        "\n",
        "_model_kwargs[\"_attn_implementation\"] = \"flash_attention_2\" if use_flash_attn else \"eager\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, **_tokenizer_kwargs)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME, **_model_kwargs)\n",
        "\n",
        "if has_cuda:\n",
        "    dtype = torch.bfloat16 if use_flash_attn else torch.float16\n",
        "    model = model.eval().cuda().to(dtype)\n",
        "else:\n",
        "    raise SystemError(\"CUDA GPU required for DeepSeek-OCR.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241a08b4",
      "metadata": {
        "id": "241a08b4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Prompt (asks model to write clean Markdown + ASCII diagrams) ===\n",
        "BASE_PROMPT = (\n",
        "    \"<image>\\n\"\n",
        "    \"<|grounding|>\"\n",
        "    \"Convert this slide into clean Markdown.\\n\"\n",
        "    \"- Keep heading hierarchy and bullet/numbered lists\\n\"\n",
        "    \"- Convert tables to Markdown tables\\n\"\n",
        "    + (\"- Reconstruct ALL diagrams as plain ASCII (no Unicode). Use + - | / \\\\ ( ) for shapes, 'o' or '*' for points, and '->' for arrows.\\n\"\n",
        "       \"  Put diagrams inside fenced code blocks like ```text ... ```.\\n\" if RECONSTRUCT_ASCII else \"\")\n",
        "    + \"- Embed any extracted figure crops using Markdown image syntax\\n\"\n",
        "    \"- Include the slide number as a level-2 heading like `## Slide {n}`\\n\"\n",
        "    \"- Output only valid Markdown\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94467731",
      "metadata": {
        "id": "94467731"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Convert & OCR ===\n",
        "from pathlib import Path\n",
        "import re, shutil\n",
        "\n",
        "in_path = Path(INPUT_FILE).expanduser().resolve()\n",
        "root_out = Path(OUTPUT_DIR)\n",
        "slides_dir = root_out / \"slides\"\n",
        "fig_out_dir = root_out / \"figures\"\n",
        "ensure_dir(root_out); ensure_dir(slides_dir); ensure_dir(fig_out_dir)\n",
        "\n",
        "# 1) Normalize to PDF\n",
        "if in_path.suffix.lower() in {\".ppt\", \".pptx\"}:\n",
        "    pdf_path = ppt_to_pdf(in_path, root_out)\n",
        "elif in_path.suffix.lower() == \".pdf\":\n",
        "    pdf_path = in_path\n",
        "else:\n",
        "    raise ValueError(\"Please provide a .ppt/.pptx/.pdf\")\n",
        "\n",
        "# 2) PDF → slide images (300 dpi for diagram fidelity)\n",
        "images = pdf_to_images(pdf_path, dpi=300, img_dir=slides_dir)\n",
        "\n",
        "# 3) OCR each slide\n",
        "markdown_chunks = []\n",
        "for idx, img in enumerate(images, 1):\n",
        "    slide_out = slides_dir / f\"slide_{idx:03d}\"\n",
        "    ensure_dir(slide_out / \"images\")\n",
        "    prompt = BASE_PROMPT.replace(\"{n}\", str(idx))\n",
        "\n",
        "    print(f\"[Slide {idx}] OCR →\", img)\n",
        "    res = model.infer(\n",
        "        tokenizer,\n",
        "        prompt=prompt,\n",
        "        image_file=str(img),\n",
        "        output_path=str(slide_out),\n",
        "        base_size=1024,            # DeepSeek-OCR \"Gundam\" preset\n",
        "        image_size=640,\n",
        "        crop_mode=True,\n",
        "        test_compress=True,\n",
        "        save_results=True\n",
        "    )\n",
        "\n",
        "    # Read the per-slide markdown produced by the model\n",
        "    result_file = slide_out / \"result.mmd\"\n",
        "    if result_file.exists():\n",
        "        md = result_file.read_text(encoding=\"utf-8\")\n",
        "\n",
        "        # Move cropped figures to a global folder and rewrite links\n",
        "        local_img_dir = slide_out / \"images\"\n",
        "        if local_img_dir.exists():\n",
        "            for i_img in sorted(local_img_dir.glob(\"*.jpg\")):\n",
        "                dst = fig_out_dir / f\"slide_{idx:03d}_{i_img.name}\"\n",
        "                shutil.copy2(i_img, dst)\n",
        "                md = md.replace(f\"images/{i_img.name}\", f\"figures/{dst.name}\")\n",
        "\n",
        "            # ASCII fallback for each crop (literal pixel→ASCII)\n",
        "            if ASCII_FOR_FIGURES:\n",
        "                ascii_blocks = []\n",
        "                for i_img in sorted(local_img_dir.glob(\"*.jpg\")):\n",
        "                    try:\n",
        "                        art = image_to_ascii(i_img, width=ASCII_FIGURE_WIDTH, charset=ASCII_CHARSET, y_aspect=ASCII_Y_ASPECT)\n",
        "                        ascii_blocks.append(f\"**ASCII fallback for figure `{i_img.name}`:**\\n\\n```text\\n{art}\\n```\")\n",
        "                    except Exception as e:\n",
        "                        ascii_blocks.append(f\"_ASCII conversion failed for {i_img.name}: {e}_\")\n",
        "                if ascii_blocks:\n",
        "                    md += \"\\n\\n\" + \"\\n\\n\".join(ascii_blocks)\n",
        "\n",
        "        # Optional ASCII of the full slide\n",
        "        if ASCII_FOR_FULL_SLIDE:\n",
        "            try:\n",
        "                full = image_to_ascii(img, width=ASCII_SLIDE_WIDTH, charset=ASCII_CHARSET, y_aspect=ASCII_Y_ASPECT)\n",
        "                md += f\"\\n\\n**ASCII fallback (full slide):**\\n\\n```text\\n{full}\\n```\"\n",
        "            except Exception as e:\n",
        "                md += f\"\\n\\n_ASCII conversion failed for full slide: {e}_\"\n",
        "\n",
        "        markdown_chunks.append(md.strip())\n",
        "    else:\n",
        "        markdown_chunks.append(f\"## Slide {idx}\\n\\n*(No text recognized)*\")\n",
        "\n",
        "# 4) Assemble master Markdown + anchors\n",
        "toc = [\"# Deck OCR\", \"## Table of Contents\"]\n",
        "for i in range(1, len(images)+1):\n",
        "    toc.append(f\"- [Slide {i}](#slide-{i})\")\n",
        "toc_md = \"\\n\".join(toc)\n",
        "\n",
        "body = []\n",
        "for i, chunk in enumerate(markdown_chunks, 1):\n",
        "    # Ensure slide anchors like \"## Slide 2\" → id=\"slide-2\"\n",
        "    chunk = re.sub(r\"^##\\s*Slide\\s+(\\d+)\", r\"## Slide \\1\\n<a id='slide-\\1'></a>\", chunk, flags=re.IGNORECASE | re.MULTILINE)\n",
        "    body.append(chunk)\n",
        "\n",
        "master = toc_md + \"\\n\\n\" + \"\\n\\n---\\n\\n\".join(body) + \"\\n\"\n",
        "out_md = Path(OUTPUT_DIR) / \"slides.md\"\n",
        "out_md.write_text(master, encoding=\"utf-8\")\n",
        "print(\"✅ Wrote:\", out_md.resolve())\n",
        "print(\"Figures dir:\", fig_out_dir.resolve())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
