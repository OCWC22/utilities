{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c044bd1",
   "metadata": {},
   "source": [
    "\n",
    "# DeepSeek-OCR: PPT/PDF → Markdown with ASCII diagrams\n",
    "\n",
    "End-to-end: **PPT/PPTX/PDF → images → DeepSeek-OCR → `slides.md`**  \n",
    "Plus **ASCII fallbacks** so figures are always visible inside Markdown (no Mermaid).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c814cb",
   "metadata": {},
   "source": [
    "\n",
    "## Quick start\n",
    "1) Set `INPUT_FILE` to your deck (`.ppt`, `.pptx`, or `.pdf`).  \n",
    "2) Run **Setup** → **Load model** → **Convert & OCR**.  \n",
    "3) Grab `output/slides.md` and any cropped images in `output/figures/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Config ===\n",
    "INPUT_FILE = \"/content/input.pdf\"   # .ppt/.pptx/.pdf\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# Use the model only for OCR/text; we still ask it to try ASCII diagrams, but\n",
    "# literal pixel→ASCII fallbacks below guarantee diagrams show up.\n",
    "RECONSTRUCT_ASCII = True             # Let the model attempt clean ASCII diagrams\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-OCR\"\n",
    "\n",
    "# Precision / memory\n",
    "USE_BF16_FLASH_ATTN = True           # Good perf on recent NVIDIA GPUs\n",
    "USE_4BIT = False                     # Set True if you need low VRAM (bitsandbytes)\n",
    "\n",
    "# ASCII fallbacks (literal pixel→ASCII) — accurate, always visible in MD\n",
    "ASCII_FOR_FIGURES = True             # ASCII for each model-cropped figure\n",
    "ASCII_FOR_FULL_SLIDE = False         # Also ASCII for whole slide image\n",
    "ASCII_FIGURE_WIDTH = 100             # 100–160 = more detail\n",
    "ASCII_SLIDE_WIDTH  = 140\n",
    "ASCII_CHARSET = \" .:-=+*#%@\"         # dark→light ramp (pure ASCII)\n",
    "ASCII_Y_ASPECT = 0.5                 # characters are taller than wide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Setup: system & Python deps ===\n",
    "# Notes:\n",
    "# - DeepSeek-OCR expects an NVIDIA GPU with CUDA + Flash-Attention 2.\n",
    "# - We install pdf2image (needs Poppler). On Linux (Colab), we apt-get poppler.\n",
    "# - For PPT/PPTX, we try headless LibreOffice to convert to PDF.\n",
    "\n",
    "import shutil, subprocess, sys\n",
    "\n",
    "def _run(cmd):\n",
    "    print(\">\", \" \".join(cmd))\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "    except Exception as e:\n",
    "        print(\"[warn]\", e)\n",
    "\n",
    "# Linux (Colab-like) helpers\n",
    "if shutil.which(\"apt-get\"):\n",
    "    _run([\"apt-get\", \"update\", \"-y\"])   # no sudo inside many notebooks\n",
    "    _run([\"apt-get\", \"install\", \"-y\", \"poppler-utils\", \"libreoffice\"])  # for pdf2image + PPT→PDF\n",
    "\n",
    "# Python deps\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install torch torchvision --index-url https://download.pytorch.org/whl/cu121 || true\n",
    "!pip -q install transformers==4.46.3 tokenizers==0.20.3 einops addict easydict pillow tqdm pdf2image python-pptx\n",
    "!pip -q install flash-attn==2.7.3 --no-build-isolation || true\n",
    "!pip -q install bitsandbytes==0.43.3 || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34012f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Utilities: PPT/PPTX → PDF, PDF → images, image → ASCII ===\n",
    "from pathlib import Path\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import shutil, subprocess\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def ppt_to_pdf(ppt_path: Path, out_dir: Path) -> Path:\n",
    "    ensure_dir(out_dir)\n",
    "    soffice = shutil.which(\"soffice\") or shutil.which(\"libreoffice\")\n",
    "    if not soffice:\n",
    "        raise RuntimeError(\"LibreOffice not found; supply a PDF directly or install LibreOffice.\")\n",
    "    cmd = [soffice, \"--headless\", \"--convert-to\", \"pdf\", \"--outdir\", str(out_dir), str(ppt_path)]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "    pdf_path = out_dir / (ppt_path.stem + \".pdf\")\n",
    "    if not pdf_path.exists():\n",
    "        alt = out_dir / (ppt_path.stem + \".PDF\")\n",
    "        if alt.exists():\n",
    "            pdf_path = alt\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(\"PPT/PPTX → PDF failed.\")\n",
    "    return pdf_path\n",
    "\n",
    "def pdf_to_images(pdf_path: Path, dpi: int = 300, img_dir: Path = Path(\"slides\")) -> list[Path]:\n",
    "    ensure_dir(img_dir)\n",
    "    pages = convert_from_path(str(pdf_path), dpi=dpi)\n",
    "    outs = []\n",
    "    for i, page in enumerate(pages, 1):\n",
    "        out = img_dir / f\"slide_{i:03d}.png\"\n",
    "        page.save(out, \"PNG\")\n",
    "        outs.append(out)\n",
    "    return outs\n",
    "\n",
    "def image_to_ascii(img_path, width=120, charset=\" .:-=+*#%@\", y_aspect=0.5):\n",
    "    im = Image.open(img_path).convert(\"L\")\n",
    "    w = int(width)\n",
    "    h = max(1, int(im.height * (w / im.width) * y_aspect))\n",
    "    im = im.resize((w, h))\n",
    "    arr = np.array(im)\n",
    "    scale = (len(charset) - 1) / 255.0\n",
    "    lines = []\n",
    "    for row in arr:\n",
    "        lines.append(\"\".join(charset[int(px * scale)] for px in row))\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Load DeepSeek-OCR ===\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "if USE_4BIT:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(\n",
    "        MODEL_NAME, trust_remote_code=True, use_safetensors=True,\n",
    "        load_in_4bit=True, device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(\n",
    "        MODEL_NAME, trust_remote_code=True, use_safetensors=True,\n",
    "        _attn_implementation=(\"flash_attention_2\" if USE_BF16_FLASH_ATTN else \"eager\")\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.eval().cuda().to(torch.bfloat16 if USE_BF16_FLASH_ATTN else torch.float16)\n",
    "    else:\n",
    "        raise SystemError(\"CUDA GPU required for DeepSeek-OCR.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Prompt (asks model to write clean Markdown + ASCII diagrams) ===\n",
    "BASE_PROMPT = (\n",
    "    \"<image>\\n\"\n",
    "    \"<|grounding|>\"\n",
    "    \"Convert this slide into clean Markdown.\\n\"\n",
    "    \"- Keep heading hierarchy and bullet/numbered lists\\n\"\n",
    "    \"- Convert tables to Markdown tables\\n\"\n",
    "    + (\"- Reconstruct ALL diagrams as plain ASCII (no Unicode). Use + - | / \\\\ ( ) for shapes, 'o' or '*' for points, and '->' for arrows.\\n\"\n",
    "       \"  Put diagrams inside fenced code blocks like ```text ... ```.\\n\" if RECONSTRUCT_ASCII else \"\")\n",
    "    + \"- Embed any extracted figure crops using Markdown image syntax\\n\"\n",
    "    \"- Include the slide number as a level-2 heading like `## Slide {n}`\\n\"\n",
    "    \"- Output only valid Markdown\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94467731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Convert & OCR ===\n",
    "from pathlib import Path\n",
    "import re, shutil\n",
    "\n",
    "in_path = Path(INPUT_FILE).expanduser().resolve()\n",
    "root_out = Path(OUTPUT_DIR)\n",
    "slides_dir = root_out / \"slides\"\n",
    "fig_out_dir = root_out / \"figures\"\n",
    "ensure_dir(root_out); ensure_dir(slides_dir); ensure_dir(fig_out_dir)\n",
    "\n",
    "# 1) Normalize to PDF\n",
    "if in_path.suffix.lower() in {\".ppt\", \".pptx\"}:\n",
    "    pdf_path = ppt_to_pdf(in_path, root_out)\n",
    "elif in_path.suffix.lower() == \".pdf\":\n",
    "    pdf_path = in_path\n",
    "else:\n",
    "    raise ValueError(\"Please provide a .ppt/.pptx/.pdf\")\n",
    "\n",
    "# 2) PDF → slide images (300 dpi for diagram fidelity)\n",
    "images = pdf_to_images(pdf_path, dpi=300, img_dir=slides_dir)\n",
    "\n",
    "# 3) OCR each slide\n",
    "markdown_chunks = []\n",
    "for idx, img in enumerate(images, 1):\n",
    "    slide_out = slides_dir / f\"slide_{idx:03d}\"\n",
    "    ensure_dir(slide_out / \"images\")\n",
    "    prompt = BASE_PROMPT.replace(\"{n}\", str(idx))\n",
    "\n",
    "    print(f\"[Slide {idx}] OCR →\", img)\n",
    "    res = model.infer(\n",
    "        tokenizer,\n",
    "        prompt=prompt,\n",
    "        image_file=str(img),\n",
    "        output_path=str(slide_out),\n",
    "        base_size=1024,            # DeepSeek-OCR \"Gundam\" preset\n",
    "        image_size=640,\n",
    "        crop_mode=True,\n",
    "        test_compress=True,\n",
    "        save_results=True\n",
    "    )\n",
    "\n",
    "    # Read the per-slide markdown produced by the model\n",
    "    result_file = slide_out / \"result.mmd\"\n",
    "    if result_file.exists():\n",
    "        md = result_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "        # Move cropped figures to a global folder and rewrite links\n",
    "        local_img_dir = slide_out / \"images\"\n",
    "        if local_img_dir.exists():\n",
    "            for i_img in sorted(local_img_dir.glob(\"*.jpg\")):\n",
    "                dst = fig_out_dir / f\"slide_{idx:03d}_{i_img.name}\"\n",
    "                shutil.copy2(i_img, dst)\n",
    "                md = md.replace(f\"images/{i_img.name}\", f\"figures/{dst.name}\")\n",
    "\n",
    "            # ASCII fallback for each crop (literal pixel→ASCII)\n",
    "            if ASCII_FOR_FIGURES:\n",
    "                ascii_blocks = []\n",
    "                for i_img in sorted(local_img_dir.glob(\"*.jpg\")):\n",
    "                    try:\n",
    "                        art = image_to_ascii(i_img, width=ASCII_FIGURE_WIDTH, charset=ASCII_CHARSET, y_aspect=ASCII_Y_ASPECT)\n",
    "                        ascii_blocks.append(f\"**ASCII fallback for figure `{i_img.name}`:**\\n\\n```text\\n{art}\\n```\")\n",
    "                    except Exception as e:\n",
    "                        ascii_blocks.append(f\"_ASCII conversion failed for {i_img.name}: {e}_\")\n",
    "                if ascii_blocks:\n",
    "                    md += \"\\n\\n\" + \"\\n\\n\".join(ascii_blocks)\n",
    "\n",
    "        # Optional ASCII of the full slide\n",
    "        if ASCII_FOR_FULL_SLIDE:\n",
    "            try:\n",
    "                full = image_to_ascii(img, width=ASCII_SLIDE_WIDTH, charset=ASCII_CHARSET, y_aspect=ASCII_Y_ASPECT)\n",
    "                md += f\"\\n\\n**ASCII fallback (full slide):**\\n\\n```text\\n{full}\\n```\"\n",
    "            except Exception as e:\n",
    "                md += f\"\\n\\n_ASCII conversion failed for full slide: {e}_\"\n",
    "\n",
    "        markdown_chunks.append(md.strip())\n",
    "    else:\n",
    "        markdown_chunks.append(f\"## Slide {idx}\\n\\n*(No text recognized)*\")\n",
    "\n",
    "# 4) Assemble master Markdown + anchors\n",
    "toc = [\"# Deck OCR\", \"## Table of Contents\"]\n",
    "for i in range(1, len(images)+1):\n",
    "    toc.append(f\"- [Slide {i}](#slide-{i})\")\n",
    "toc_md = \"\\n\".join(toc)\n",
    "\n",
    "body = []\n",
    "for i, chunk in enumerate(markdown_chunks, 1):\n",
    "    # Ensure slide anchors like \"## Slide 2\" → id=\"slide-2\"\n",
    "    chunk = re.sub(r\"^##\\s*Slide\\s+(\\d+)\", r\"## Slide \\1\\n<a id='slide-\\1'></a>\", chunk, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    body.append(chunk)\n",
    "\n",
    "master = toc_md + \"\\n\\n\" + \"\\n\\n---\\n\\n\".join(body) + \"\\n\"\n",
    "out_md = Path(OUTPUT_DIR) / \"slides.md\"\n",
    "out_md.write_text(master, encoding=\"utf-8\")\n",
    "print(\"✅ Wrote:\", out_md.resolve())\n",
    "print(\"Figures dir:\", fig_out_dir.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
